{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxic Comment Classification Using Deep Learning\n",
    "\n",
    "Study of negative online behaviour. We need to be able to classify text and discern between different levels of toxicity and disrespectful behaviour - such as like threats, obscenity, insults, and identity-based hate etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...      0   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...      0   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...      0   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...      0   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "raw_data = pd.read_csv('../data/train.csv', encoding=\"utf-8\")\n",
    "display(raw_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "\n",
    "Vectorize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import sklearn\n",
    "import gensim\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from sklearn.manifold import TSNE\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt                        \n",
    "%matplotlib inline\n",
    "\n",
    "class vectorizer(object):\n",
    "    def __init__(self, vector_method, size, sg, window, min_count, seed, workers, sents):\n",
    "        self._vector_method = vector_method\n",
    "        self._size = size\n",
    "        self._sg = sg\n",
    "        self._window = window\n",
    "        self._min_count = min_count\n",
    "        self._seed = seed\n",
    "        self._sents = sents\n",
    "        self._workers = workers\n",
    "        \n",
    "    def vectorize_model(self):\n",
    "        model = self._vector_method(sentences=self._sents, size=self._size, sg=self._sg,\n",
    "                 window=self._window, min_count=self._min_count, seed=self._seed,\n",
    "                 workers=self._workers)\n",
    "        \n",
    "        return model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done processing\n"
     ]
    }
   ],
   "source": [
    "# print(raw_data['comment_text'][:2])\n",
    "def parse_sentences():\n",
    "    sentences = []\n",
    "    for index, row in raw_data.iterrows():\n",
    "        text = row['comment_text']\n",
    "        sent = list(sent_tokenize(text.lower()))\n",
    "        sentences.append(sent)\n",
    "\n",
    "    return sentences\n",
    "\n",
    "def word_process(some_text):\n",
    "    stop_words = set(stopwords.words('english'))    \n",
    "    words = word_tokenize(some_text[0].lower())\n",
    "    \n",
    "    filtered_words = []\n",
    "    for w in words:\n",
    "        if w not in stop_words:\n",
    "            filtered_words.append(w)\n",
    "            \n",
    "    return list(words) \n",
    "\n",
    "def parse_raw():\n",
    "    documents = []\n",
    "    sents = parse_sentences()\n",
    "    for sent in sents:\n",
    "        processed = word_process(sent)\n",
    "        documents.append(processed)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "sentences = parse_raw()\n",
    "# print(sentences)\n",
    "\n",
    "word2vec_method = vectorizer(Word2Vec, 64, 1, 10, 3, 42, 2, sentences)\n",
    "word2vec_model = word2vec_method.vectorize_model()\n",
    "print('done processing')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(word2vec_model.wv.vocab)\n",
    "# print(word2vec_model['fuck'])\n",
    "print(word2vec_model.most_similar('fuck'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2, n_iter=300)\n",
    "\n",
    "X = word2vec_model[word2vec_model.wv.vocab]\n",
    "X_2d = tsne.fit_transform(X)\n",
    "coords_df = pd.DataFrame(X_2d, columns=['x', 'y'])\n",
    "coords_df['token'] = word2vec_model.wv.vocab.keys()\n",
    "\n",
    "print(coords_df.head())\n",
    "\n",
    "# Plot the graph.\n",
    "coords_df.plot.scatter('x', 'y', figsize=(8,8),\n",
    "                       marker='o', s=10, alpha=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We define the features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK', 'Hey... what is it..\\n@ | talk .\\nWhat is it... an exclusive group of some WP TALIBANS...who are good at destroying, self-appointed purist who GANG UP any one who asks them questions abt their ANTI-SOCIAL and DESTRUCTIVE (non)-contribution at WP?\\n\\nAsk Sityush to clean up his behavior than issue me nonsensical warnings...']\n",
      "[[1 1 0 1 0]\n",
      " [0 0 0 0 0]\n",
      " [0 0 0 0 0]\n",
      " [0 1 0 1 1]\n",
      " [0 1 0 1 0]\n",
      " [0 0 0 0 0]]\n",
      "15294\n"
     ]
    }
   ],
   "source": [
    "# clean up the docs and labels a bit\n",
    "all_docs = list(raw_data['comment_text'])\n",
    "all_labels = np.array(list(raw_data['toxic']))\n",
    "\n",
    "def retrieve_toxic_data():\n",
    "    toxic_docs = []\n",
    "    toxic_labels = []\n",
    "    for index, row in raw_data.iterrows():\n",
    "        if int(row['toxic']) == 1:\n",
    "            toxic_docs.append(row['comment_text'])\n",
    "            toxic_labels.append(raw_data.iloc[index,3:])\n",
    "            \n",
    "    return toxic_docs, np.array(toxic_labels), len(toxic_docs)\n",
    "\n",
    "toxic_docs, toxic_labels, toxic_entries = retrieve_toxic_data()    \n",
    "\n",
    "print(toxic_docs[:2])\n",
    "print(toxic_labels[:6])\n",
    "print(toxic_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed-forward neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import pickle\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Dropout, Dense, Flatten\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "class neural_network(object):\n",
    "    def __init__(self, docs, labels, epochs, batch_size, dropout):\n",
    "        self._model = Sequential()\n",
    "        self._docs = docs\n",
    "        self._labels = labels\n",
    "        self._epochs = epochs\n",
    "        self._batch_size = batch_size\n",
    "        self._dropout = dropout\n",
    "     \n",
    "    def split_data(self, docs, targets):\n",
    "        length = len(docs)\n",
    "        split_point = int(round(length * 0.8))\n",
    "        return docs[:split_point], targets[:split_point], docs[split_point:], targets[split_point:]\n",
    "        \n",
    "    def prepare_docs(self, max_length):\n",
    "        t = Tokenizer(num_words=5000)\n",
    "        t.fit_on_texts(self._docs)\n",
    "        vocab_size = len(t.word_index) + 1\n",
    "        encoded_docs = t.texts_to_sequences(self._docs)\n",
    "        padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "        return padded_docs, t\n",
    "    \n",
    "    # only put in top 5000 words?\n",
    "    def calc_embedding_matrix(self, vocab_size, t):\n",
    "        embedding_matrix = np.zeros((vocab_size, 64))\n",
    "        for word, i in t.word_index.items():\n",
    "            if word in word2vec_model.wv.vocab:\n",
    "                embedding_vector = word2vec_model.wv[word]\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "                \n",
    "        return embedding_matrix\n",
    "    \n",
    "    def construct_model(self, vocab_size, max_length, embedding_matrix):\n",
    "        \n",
    "        embedding_layer = Embedding(vocab_size, 64, weights=[embedding_matrix], input_length=max_length)\n",
    "        embedding_layer.trainable = False\n",
    "        \n",
    "        self._model.add(embedding_layer)\n",
    "        self._model.add(Flatten())\n",
    "        self._model.add(Dense(64, activation='relu'))\n",
    "        self._model.add(Dense(64, activation='relu'))\n",
    "        self._model.add(Dropout(self._dropout))\n",
    "        self._model.add(Dense(1, activation='sigmoid'))\n",
    "        self._model.summary()\n",
    "        \n",
    "    def compile_model(self):\n",
    "        sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "        self._model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        checkpointer = ModelCheckpoint(filepath='saved_models/weights.feed_forward_network.hdf5', \n",
    "                               verbose=1, save_best_only=True)\n",
    "        \n",
    "        return checkpointer\n",
    "        \n",
    "    def train_save_model(self, train_docs, train_targets, valid_docs, valid_targets, checkpointer):\n",
    "        trained_model = self._model.fit(train_docs, train_targets, validation_data=(valid_docs, valid_targets), \n",
    "                          epochs=self._epochs, batch_size=self._batch_size, callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "        with open('pickles/feed_forward_network', 'wb') as file_pi:\n",
    "            pickle.dump(trained_model.history, file_pi)\n",
    "            \n",
    "        return trained_model\n",
    "            \n",
    "    def get_model(self):\n",
    "        return self._model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_forward_network = neural_network(all_docs, all_labels, 8, 32, 0.5)\n",
    "padded_docs, t = feed_forward_network.prepare_docs(150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = feed_forward_network.calc_embedding_matrix(len(t.word_index) + 1, t)\n",
    "# print(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_docs, train_valid_targets, test_docs, test_targets = feed_forward_network.split_data(padded_docs, all_labels)\n",
    "train_docs, train_targets, valid_docs, valid_targets = feed_forward_network.split_data(train_valid_docs, train_valid_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feed_forward_network.construct_model(len(t.word_index) + 1, 150, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = feed_forward_network.compile_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_feed_forward_model = feed_forward_network.train_save_model(train_docs, train_targets, valid_docs, valid_targets, checkpointer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the predictions and evaluate the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "\n",
    "# test_docs, test_targets\n",
    "\n",
    "class predictive_evaluate(object):\n",
    "    def __init__(self, test_docs, test_targets, model):\n",
    "        self._test_docs = test_docs\n",
    "        self._test_targets = test_targets\n",
    "        self._model = model\n",
    "        \n",
    "    def get_predictions(self):\n",
    "        predictions = []\n",
    "        for doc in self._test_docs:\n",
    "            result = self._model.predict(np.expand_dims(doc, axis=0))\n",
    "            preds = result[0][0]\n",
    "            predictions.append(1 if preds >= 0.5 else 0)\n",
    "                    \n",
    "        return predictions\n",
    "    \n",
    "    def get_multilabel_predictions(self):\n",
    "        predictions = []\n",
    "        for doc in self._test_docs:\n",
    "            result = self._model.predict(np.expand_dims(doc, axis=0))\n",
    "            preds = result[0]\n",
    "            preds[preds >= 0.5] = 1\n",
    "            preds[preds < 0.5] = 0 \n",
    "            predictions.append(preds)\n",
    "                    \n",
    "        return predictions\n",
    "        \n",
    "    def get_metrics(self, predictions):\n",
    "        # print(self._test_targets[0])\n",
    "        # print(predictions[0])\n",
    "        model_recall_score = recall_score(self._test_targets, predictions, average='weighted')\n",
    "        model_precision_score = precision_score(self._test_targets, predictions, average='weighted')\n",
    "        model_f1_score = f1_score(self._test_targets, predictions, average='weighted')\n",
    "        \n",
    "        return model_recall_score, model_precision_score, model_f1_score\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = feed_forward_network.get_model()\n",
    "model.load_weights('saved_models/weights.feed_forward_network.hdf5')\n",
    "\n",
    "evaluation = predictive_evaluate(test_docs, test_targets, model)\n",
    "predictions = np.array(evaluation.get_predictions()).astype(int)\n",
    "\n",
    "print(predictions[100:110])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_recall_score = recall_score(np.array(test_targets), predictions, average='weighted')\n",
    "model_precision_score = precision_score(np.array(test_targets), predictions, average='weighted')\n",
    "model_fbeta_score = fbeta_score(np.array(test_targets), predictions, average='weighted', beta=1)\n",
    "\n",
    "print('Recall score:', model_recall_score)\n",
    "print('Precision score:', model_precision_score)\n",
    "print('F-beta score:', model_fbeta_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try a convolutional neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, SpatialDropout1D, GlobalMaxPool1D, MaxPooling1D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "class cnn_neural_network(neural_network):\n",
    "    def __init__(self, docs, labels, epochs, batch_size, dropout, pad, drop_embed, n_conv, k_conv):\n",
    "        super(cnn_neural_network, self).__init__(docs, labels, epochs, batch_size, dropout)\n",
    "        self._pad = pad\n",
    "        self._drop_embed = drop_embed\n",
    "        self._n_conv = n_conv\n",
    "        self._k_conv = k_conv\n",
    "        \n",
    "    def prepare_docs(self, max_length):\n",
    "        t = Tokenizer(num_words=5000)\n",
    "        t.fit_on_texts(self._docs)\n",
    "        vocab_size = len(t.word_index) + 1\n",
    "        encoded_docs = t.texts_to_sequences(self._docs)\n",
    "        padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding=self._pad, truncating=self._pad, value=0)\n",
    "        return padded_docs, t\n",
    "    \n",
    "    def construct_model(self, vocab_size, max_length, embedding_matrix):\n",
    "        \n",
    "        embedding_layer = Embedding(vocab_size, 64, weights=[embedding_matrix], input_length=max_length)\n",
    "        embedding_layer.trainable = False\n",
    "        \n",
    "        self._model.add(embedding_layer)\n",
    "        self._model.add(SpatialDropout1D(self._drop_embed))\n",
    "        self._model.add(Conv1D(self._n_conv, self._k_conv, activation='relu'))\n",
    "        self._model.add(MaxPooling1D(pool_size = 2))\n",
    "        self._model.add(Conv1D(self._n_conv * 2, self._k_conv, activation='relu'))\n",
    "        self._model.add(MaxPooling1D(pool_size = 2))\n",
    "        self._model.add(Conv1D(self._n_conv * 4, self._k_conv, activation='relu'))\n",
    "        self._model.add(GlobalMaxPool1D())\n",
    "        self._model.add(Dense(self._n_conv, activation='relu'))\n",
    "        self._model.add(Dropout(self._dropout))\n",
    "        self._model.add(Dense(1, activation='sigmoid'))\n",
    "        self._model.summary()\n",
    "        \n",
    "    def construct_toxic_model(self, vocab_size, max_length, embedding_matrix):\n",
    "        \n",
    "        embedding_layer = Embedding(vocab_size, 64, weights=[embedding_matrix], input_length=max_length)\n",
    "        embedding_layer.trainable = False\n",
    "        \n",
    "        self._model.add(embedding_layer)\n",
    "        self._model.add(SpatialDropout1D(self._drop_embed))\n",
    "        self._model.add(Conv1D(self._n_conv, self._k_conv, activation='relu'))\n",
    "        self._model.add(MaxPooling1D(pool_size = 2))\n",
    "        self._model.add(Conv1D(self._n_conv * 2, self._k_conv, activation='relu'))\n",
    "        self._model.add(GlobalMaxPool1D())\n",
    "        self._model.add(Dense(self._n_conv, activation='relu'))\n",
    "        self._model.add(Dropout(self._dropout))\n",
    "        self._model.add(Dense(1, activation='sigmoid'))\n",
    "        self._model.summary()\n",
    "        \n",
    "    def compile_model(self, model_name):\n",
    "        sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "        self._model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "        checkpointer = ModelCheckpoint(filepath='saved_models/' + model_name, \n",
    "                               verbose=1, save_best_only=True)\n",
    "        \n",
    "        return checkpointer\n",
    "        \n",
    "    def train_save_model(self, train_docs, train_targets, valid_docs, valid_targets, checkpointer, model_name):\n",
    "        trained_model = self._model.fit(train_docs, train_targets, validation_data=(valid_docs, valid_targets), \n",
    "                          epochs=self._epochs, batch_size=self._batch_size, callbacks=[checkpointer], verbose=1)\n",
    "\n",
    "        with open('pickles/cnn_network' + model_name, 'wb') as file_pi:\n",
    "            pickle.dump(trained_model.history, file_pi)\n",
    "            \n",
    "        return trained_model\n",
    "            \n",
    "    def get_model(self):\n",
    "        return self._model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bec22a677e0afa79</td>\n",
       "      <td>\"\\n\\n Please do not intentionally introduce in...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5b032165008c0ec0</td>\n",
       "      <td>\"\\n\\n Niggard PLEASE! \\n\\nThe niggardly nigger...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a3473233fd5ab0d6</td>\n",
       "      <td>Vijayawada\\nThe edit here seems to be an assum...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7219fe942714bc03</td>\n",
       "      <td>Thanks for noting this here.</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a3ada67e46304a47</td>\n",
       "      <td>\"\\nRe: question posed to Shell about my commen...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  bec22a677e0afa79  \"\\n\\n Please do not intentionally introduce in...      0   \n",
       "1  5b032165008c0ec0  \"\\n\\n Niggard PLEASE! \\n\\nThe niggardly nigger...      1   \n",
       "2  a3473233fd5ab0d6  Vijayawada\\nThe edit here seems to be an assum...      0   \n",
       "3  7219fe942714bc03                       Thanks for noting this here.      0   \n",
       "4  a3ada67e46304a47  \"\\nRe: question posed to Shell about my commen...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        1       1       1              1  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>59571.000000</td>\n",
       "      <td>59571.000000</td>\n",
       "      <td>59571.000000</td>\n",
       "      <td>59571.000000</td>\n",
       "      <td>59571.000000</td>\n",
       "      <td>59571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.256736</td>\n",
       "      <td>0.026775</td>\n",
       "      <td>0.135704</td>\n",
       "      <td>0.007705</td>\n",
       "      <td>0.126051</td>\n",
       "      <td>0.022377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.436836</td>\n",
       "      <td>0.161426</td>\n",
       "      <td>0.342476</td>\n",
       "      <td>0.087441</td>\n",
       "      <td>0.331910</td>\n",
       "      <td>0.147906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              toxic  severe_toxic       obscene        threat        insult  \\\n",
       "count  59571.000000  59571.000000  59571.000000  59571.000000  59571.000000   \n",
       "mean       0.256736      0.026775      0.135704      0.007705      0.126051   \n",
       "std        0.436836      0.161426      0.342476      0.087441      0.331910   \n",
       "min        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "75%        1.000000      0.000000      0.000000      0.000000      0.000000   \n",
       "max        1.000000      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "       identity_hate  \n",
       "count   59571.000000  \n",
       "mean        0.022377  \n",
       "std         0.147906  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# downsampling\n",
    "def downsample(docs, labels):\n",
    "    downsample_index = 0\n",
    "    for index, label in enumerate(labels):\n",
    "        if int(label) == 1 and int(labels[index + 1]) == 0:\n",
    "            del docs[index + 1]\n",
    "            del labels[index + 1]\n",
    "            \n",
    "    return docs, np.array(labels) , len(docs)\n",
    "\n",
    "def filter_df(filter_limit, raw_df):\n",
    "    dataframe = raw_df.copy()\n",
    "    dataframe = dataframe.sort_values('toxic', ascending=True)\n",
    "    dataframe = dataframe.reset_index(drop=True)\n",
    "    dataframe = dataframe.iloc[filter_limit:]\n",
    "\n",
    "    return dataframe.sample(frac=1).reset_index(drop=True)\n",
    "        \n",
    "dataframe = filter_df(100000, raw_data)\n",
    "display(dataframe.head())\n",
    "display(dataframe.describe())\n",
    "\n",
    "shrink_docs = list(dataframe['comment_text'])\n",
    "shrink_labels = np.array(list(dataframe['toxic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25673565996877673\n",
      "0.25596705660179403\n",
      "0.2566728218902132\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 1500, 64)          7041472   \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 1500, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 1498, 256)         49408     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 749, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 747, 512)          393728    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 373, 512)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 371, 1024)         1573888   \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               262400    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 9,321,153.0\n",
      "Trainable params: 2,279,681.0\n",
      "Non-trainable params: 7,041,472.0\n",
      "_________________________________________________________________\n",
      "Train on 38126 samples, validate on 9531 samples\n",
      "Epoch 1/8\n",
      "38112/38126 [============================>.] - ETA: 0s - loss: 0.4254 - acc: 0.8131Epoch 00000: val_loss improved from inf to 0.23037, saving model to saved_models/cnn_network_binary.hdf5\n",
      "38126/38126 [==============================] - 186s - loss: 0.4254 - acc: 0.8131 - val_loss: 0.2304 - val_acc: 0.9097\n",
      "Epoch 2/8\n",
      "38112/38126 [============================>.] - ETA: 0s - loss: 0.2354 - acc: 0.9056Epoch 00001: val_loss did not improve\n",
      "38126/38126 [==============================] - 183s - loss: 0.2354 - acc: 0.9056 - val_loss: 0.2488 - val_acc: 0.9037\n",
      "Epoch 3/8\n",
      "38112/38126 [============================>.] - ETA: 0s - loss: 0.2253 - acc: 0.9088Epoch 00002: val_loss improved from 0.23037 to 0.20174, saving model to saved_models/cnn_network_binary.hdf5\n",
      "38126/38126 [==============================] - 182s - loss: 0.2253 - acc: 0.9088 - val_loss: 0.2017 - val_acc: 0.9211\n",
      "Epoch 4/8\n",
      "38112/38126 [============================>.] - ETA: 0s - loss: 0.2179 - acc: 0.9111Epoch 00003: val_loss did not improve\n",
      "38126/38126 [==============================] - 182s - loss: 0.2179 - acc: 0.9111 - val_loss: 0.2037 - val_acc: 0.9232\n",
      "Epoch 5/8\n",
      "38112/38126 [============================>.] - ETA: 0s - loss: 0.2117 - acc: 0.9161Epoch 00004: val_loss did not improve\n",
      "38126/38126 [==============================] - 182s - loss: 0.2116 - acc: 0.9161 - val_loss: 0.2070 - val_acc: 0.9212\n",
      "Epoch 6/8\n",
      "38112/38126 [============================>.] - ETA: 0s - loss: 0.2074 - acc: 0.9172Epoch 00005: val_loss improved from 0.20174 to 0.19744, saving model to saved_models/cnn_network_binary.hdf5\n",
      "38126/38126 [==============================] - 182s - loss: 0.2074 - acc: 0.9172 - val_loss: 0.1974 - val_acc: 0.9251\n",
      "Epoch 7/8\n",
      "38112/38126 [============================>.] - ETA: 0s - loss: 0.2035 - acc: 0.9171Epoch 00006: val_loss did not improve\n",
      "38126/38126 [==============================] - 182s - loss: 0.2035 - acc: 0.9171 - val_loss: 0.2016 - val_acc: 0.9210\n",
      "Epoch 8/8\n",
      "38112/38126 [============================>.] - ETA: 0s - loss: 0.1983 - acc: 0.9202Epoch 00007: val_loss improved from 0.19744 to 0.19318, saving model to saved_models/cnn_network_binary.hdf5\n",
      "38126/38126 [==============================] - 182s - loss: 0.1983 - acc: 0.9202 - val_loss: 0.1932 - val_acc: 0.9261\n",
      "Recall score: [0.96409214 0.81262263]\n",
      "Precision score: [0.93710899 0.88655012]\n",
      "F-beta score: 0.9241178493678874\n"
     ]
    }
   ],
   "source": [
    "# epochs = 15\n",
    "# batch_size = 128\n",
    "# dropout = 0.5\n",
    "# pad = 'pre'\n",
    "# skip = 50\n",
    "# drop_embed = 0.2\n",
    "# n_conv = 256\n",
    "# k_conv = 3\n",
    "\n",
    "cnn_network = cnn_neural_network(shrink_docs, shrink_labels, 8, 16, 0.5, 'post', 0.2, 256, 3)\n",
    "padded_cnn_docs, t = cnn_network.prepare_docs(1500)\n",
    "embedding_matrix = cnn_network.calc_embedding_matrix(len(t.word_index) + 1, t)\n",
    "\n",
    "train_valid_docs, train_valid_targets, test_docs, test_targets = cnn_network.split_data(padded_cnn_docs, shrink_labels)\n",
    "train_docs, train_targets, valid_docs, valid_targets = cnn_network.split_data(train_valid_docs, train_valid_targets)\n",
    "\n",
    "def occurence_of_toxic(targets):\n",
    "    occurence = 0\n",
    "    for label in targets:\n",
    "        if label == 1:\n",
    "            occurence += 1\n",
    "            \n",
    "    return occurence / len(targets)\n",
    "\n",
    "print(occurence_of_toxic(shrink_labels))\n",
    "print(occurence_of_toxic(train_targets))\n",
    "print(occurence_of_toxic(test_targets))\n",
    "\n",
    "cnn_network.construct_model(len(t.word_index) + 1, 1500, embedding_matrix)\n",
    "checkpointer = cnn_network.compile_model('cnn_network_binary.hdf5')\n",
    "trained_cnn_model = cnn_network.train_save_model(train_docs, train_targets, valid_docs, valid_targets, checkpointer, 'cnn_network_binary')\n",
    "\n",
    "model = cnn_network.get_model()\n",
    "model.load_weights('saved_models/cnn_network_binary.hdf5')\n",
    "\n",
    "evaluation = predictive_evaluate(test_docs, test_targets, model)\n",
    "predictions = np.array(evaluation.get_predictions()).astype(int)\n",
    "\n",
    "model_recall_score = recall_score(np.array(test_targets), predictions, average=None)\n",
    "model_precision_score = precision_score(np.array(test_targets), predictions, average=None)\n",
    "model_fbeta_score = fbeta_score(np.array(test_targets), predictions, average='weighted', beta=1)\n",
    "\n",
    "print('Recall score:', model_recall_score)\n",
    "print('Precision score:', model_precision_score)\n",
    "print('F-beta score:', model_fbeta_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's use a LSTM Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10428926376356741 1595\n",
      "0.5182424480188309 7926\n",
      "0.02935791813783183 449\n",
      "0.48018830914083954 7344\n",
      "0.0851314240878776 1302\n"
     ]
    }
   ],
   "source": [
    "# Can we build binary classification models for more specific categories?\n",
    "def occurence_of_labels(label_pos, labels):\n",
    "    occurence = 0\n",
    "    for label in labels:\n",
    "        if int(label[label_pos]) == 1:\n",
    "            occurence += 1\n",
    "    \n",
    "    return occurence / len(labels), occurence\n",
    "\n",
    "frequency, total = occurence_of_labels(0, toxic_labels)\n",
    "print(frequency, total)\n",
    "frequency, total = occurence_of_labels(1, toxic_labels)\n",
    "print(frequency, total)\n",
    "frequency, total = occurence_of_labels(2, toxic_labels)\n",
    "print(frequency, total)\n",
    "frequency, total = occurence_of_labels(3, toxic_labels)\n",
    "print(frequency, total)\n",
    "frequency, total = occurence_of_labels(4, toxic_labels)\n",
    "print(frequency, total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]]\n",
      "0    Explanation\\nWhy the edits made under my usern...\n",
      "1    D'aww! He matches this background colour I'm s...\n",
      "Name: comment_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "list_labels = raw_data[list_classes].values\n",
    "list_docs = raw_data[\"comment_text\"]\n",
    "\n",
    "print(list_labels[:2])\n",
    "print(list_docs[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[688, 75, 1, 126, 130, 177, 29, 672, 4531, 12139, 1118, 86, 331, 51, 2279, 11648, 50, 6884, 15, 60, 2758, 148, 7, 2950, 34, 117, 1221, 15532, 2832, 4, 45, 59, 244, 1, 365, 31, 1, 38, 27, 144, 73, 3466, 89, 3088, 4593, 2273, 987], [52, 2636, 13, 555, 3814, 73, 4581, 2706, 21, 94, 38, 804, 2682, 992, 589, 8394, 182]]\n"
     ]
    }
   ],
   "source": [
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_docs))\n",
    "list_tokenized_docs = tokenizer.texts_to_sequences(list_docs)\n",
    "\n",
    "print(list_tokenized_docs[:2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0   688    75     1\n",
      "    126   130   177    29   672  4531 12139  1118    86   331    51  2279\n",
      "  11648    50  6884    15    60  2758   148     7  2950    34   117  1221\n",
      "  15532  2832     4    45    59   244     1   365    31     1    38    27\n",
      "    144    73  3466    89  3088  4593  2273   987]\n",
      " [    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0    52  2636    13   555  3814    73  4581  2706    21\n",
      "     94    38   804  2682   992   589  8394   182]]\n"
     ]
    }
   ],
   "source": [
    "# Use padding for consistency in length\n",
    "maxlen = 200\n",
    "X_t = pad_sequences(list_tokenized_docs, maxlen=maxlen)\n",
    "\n",
    "print(X_t[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAD8CAYAAACcjGjIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngySpYzhIkjqGgySpYzhIkjr/HzOS8B9zilX2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f134713afd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Have a look at the distribution of number of words in each doc\n",
    "totalNumWords = [len(one_comment) for one_comment in list_tokenized_docs]\n",
    "plt.hist(totalNumWords, bins = np.arange(0,410,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(docs, targets):\n",
    "        length = len(docs)\n",
    "        split_point = int(round(length * 0.8))\n",
    "        return docs[:split_point], targets[:split_point], docs[split_point:], targets[split_point:]\n",
    "    \n",
    "train_valid_docs, train_valid_targets, test_docs, test_targets = split_data(X_t, list_labels)\n",
    "train_docs, train_targets, valid_docs, valid_targets = split_data(train_valid_docs, train_valid_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 200, 128)          2560000   \n",
      "_________________________________________________________________\n",
      "lstm_layer (LSTM)            (None, 200, 60)           45360     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                3050      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 2,608,716.0\n",
      "Trainable params: 2,608,716.0\n",
      "Non-trainable params: 0.0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Tell keras to infer the input shape, in our case it is 200\n",
    "\n",
    "# We construct the LSTM network here\n",
    "\n",
    "inp = Input(shape=(maxlen, )) #maxlen=200 as defined earlier\n",
    "embed_size = 128\n",
    "x = Embedding(max_features, embed_size)(inp)\n",
    "x = LSTM(60, return_sequences=True,name='lstm_layer')(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(50, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(6, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 102126 samples, validate on 25531 samples\n",
      "Epoch 1/2\n",
      "102112/102126 [============================>.] - ETA: 0s - loss: 0.0762 - acc: 0.9762Epoch 00000: val_loss improved from inf to 0.04989, saving model to saved_models/LSTM.hdf5\n",
      "102126/102126 [==============================] - 1040s - loss: 0.0762 - acc: 0.9762 - val_loss: 0.0499 - val_acc: 0.9820\n",
      "Epoch 2/2\n",
      "102112/102126 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9829Epoch 00001: val_loss did not improve\n",
      "102126/102126 [==============================] - 1034s - loss: 0.0465 - acc: 0.9829 - val_loss: 0.0513 - val_acc: 0.9816\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "epochs = 2\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath='saved_models/LSTM.hdf5', verbose=1, save_best_only=True)\n",
    "\n",
    "model.fit(train_docs, train_targets, validation_data=(valid_docs, valid_targets), \n",
    "        epochs=epochs, batch_size=batch_size, callbacks=[checkpointer], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checkpoint\n",
      "31914\n",
      "31914\n",
      "[[0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [1 0 1 0 1 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [1 0 1 0 1 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.4/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib64/python3.4/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('saved_models/LSTM.hdf5')\n",
    "\n",
    "evaluation = predictive_evaluate(test_docs, test_targets, model)\n",
    "print('checkpoint')\n",
    "print(len(test_docs))\n",
    "print(len(test_targets))\n",
    "predictions = np.array(evaluation.get_multilabel_predictions()).astype(int)\n",
    "\n",
    "print(predictions[200:230])\n",
    "\n",
    "model_recall_score = recall_score(np.array(test_targets), predictions, average=None)\n",
    "model_precision_score = precision_score(np.array(test_targets), predictions, average=None)\n",
    "model_fbeta_score = fbeta_score(np.array(test_targets), predictions, average='weighted', beta=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.57853145 0.07395498 0.65787897 0.         0.52591656 0.        ]\n",
      "[0.68928992 0.63888889 0.78372591 0.         0.71724138 0.        ]\n",
      "0.5868524223977268\n"
     ]
    }
   ],
   "source": [
    "print(model_recall_score)\n",
    "print(model_precision_score)\n",
    "print(model_fbeta_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
